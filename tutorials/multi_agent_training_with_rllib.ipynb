{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.  \n",
    "All rights reserved.  \n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab\n",
    "\n",
    "Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/ai-economist/blob/master/tutorials/multi_agent_training_with_rllib.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "It is helpful to be familiar with **Foundation**, a multi-agent economic simulator built for the AI Economist ([paper here](https://arxiv.org/abs/2004.13332)). If you haven't worked with Foundation before, we highly recommend taking a look at our other tutorials:\n",
    "\n",
    "- [Foundation: the Basics](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb)\n",
    "- [Extending Foundation](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_advanced.ipynb)\n",
    "- [Optimal Taxation Theory and Simulation](https://github.com/salesforce/ai-economist/blob/master/tutorials/optimal_taxation_theory_and_simulation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! This tutorial is the first of a series on doing distributed multi-agent reinforcement learning (MARL). Here, we specifically demonstrate how to integrate our multi-agent economic simulation, [Foundation](https://github.com/salesforce/ai-economist/tree/master/ai_economist/foundation), with [RLlib](https://github.com/ray-project/ray/tree/master/rllib), an open-source library for reinforcement learning. We chose to use RLlib, as it provides an easy-to-use and flexible library for MARL. A detailed documentation on RLlib is available [here](https://docs.ray.io/en/master/rllib.html).\n",
    "\n",
    "We put together these tutorial notebook with the following key goals in mind:\n",
    "- Provide an exposition to MARL. While there are many libraries and references out there for single-agent RL training, MARL training is not discussed as much, and there aren't many multi-agent rl libraries.\n",
    "- Provide reference starting code to perform MARL training so the AI Economist community can focus more on building meaningful extensions to Foundation and better-performant algorithms.\n",
    "\n",
    "We will cover the following concepts in this tutorial:\n",
    "1. Adding an *environment wrapper* to make the economic simulation compatible with RLlib.\n",
    "2. Creating a *trainer* object that holds the (multi-agent) policies for environment interaction.\n",
    "3. Training all the agents in the economic simulation.\n",
    "4. Generate a rollout using the trainer object and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies:\n",
    "You can install the ai-economist package using \n",
    "- the pip package manager OR\n",
    "- by cloning the ai-economist package and installing the requirements (we shall use this when running on Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, signal, sys, time\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/salesforce/ai-economist.git\n",
    "\n",
    "    %cd ai-economist\n",
    "    !pip install -e .\n",
    "    \n",
    "    # Restart the Python runtime to automatically use the installed packages\n",
    "    print(\"\\n\\nRestarting the Python runtime! Please (re-)run the cells below.\")\n",
    "    time.sleep(1)\n",
    "    os.kill(os.getpid(), signal.SIGKILL)\n",
    "else:\n",
    "    ! pip install ai-economist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install OpenAI Gym to help define the environment's observation and action spaces for use with RLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym==0.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the `RLlib` reinforcement learning library:\n",
    "- First, install TensorFlow\n",
    "- Then, install ray[rllib]\n",
    "\n",
    "Note: RLlib natively supports TensorFlow (including TensorFlow Eager) as well as PyTorch, but most of its internals are framework agnostic. Here's a relevant [blogpost](https://medium.com/distributed-computing-with-ray/lessons-from-implementing-12-deep-rl-algorithms-in-tf-and-pytorch-1b412009297d) that compares running RLlib algorithms with TF and PyTorch. Overall, TF seems to run a bit faster than PyTorch, in our experience, and we will use that in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We install these specific versions of tensorflow and rllib, that we used in our work.\n",
    "!pip install tensorflow==1.14\n",
    "!pip install \"ray[rllib]==0.8.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to the tutorials folder\n",
    "import os, sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\"/content/ai-economist/tutorials\")\n",
    "else:\n",
    "    os.chdir(os.path.dirname(os.path.abspath(\"__file__\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Adding an Environment Wrapper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a configuration (introduced in [the basics tutorial](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basics.ipynb)) for the \"gather-trade-build\" environment with multiple mobile agents (that move, gather resources, build or trade) and a social planner that sets taxes according to (a scaled variant of) the 2018 US tax schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside covid19_components.py: 0 GPUs are available.\n",
      "No GPUs found! Running the simulation on a CPU.\n"
     ]
    }
   ],
   "source": [
    "from ai_economist import foundation\n",
    "\n",
    "from ai_economist.foundation.base.base_env import BaseEnvironment, scenario_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a configuration (dictionary) for the \"egoistic-democratic-one-step-economoy\" environment.\n",
    "\n",
    "env_config_dict = {\n",
    "    # ===== SCENARIO CLASS =====\n",
    "    # Which Scenario class to use: the class's name in the Scenario Registry (foundation.scenarios).\n",
    "    # The environment object will be an instance of the Scenario class.\n",
    "    'scenario_name': 'considerate-democratic-one-step-economy',\n",
    "    \n",
    "    # ===== COMPONENTS =====\n",
    "    # Which components to use (specified as list of (\"component_name\", {component_kwargs}) tuples).\n",
    "    #   \"component_name\" refers to the Component class's name in the Component Registry (foundation.components)\n",
    "    #   {component_kwargs} is a dictionary of kwargs passed to the Component class\n",
    "    # The order in which components reset, step, and generate obs follows their listed order below.\n",
    "    'components': [\n",
    "        # (1) SimpleLabor\n",
    "        ('SimpleLabor', {\n",
    "            'mask_first_step': True,\n",
    "            'pareto_param': 8,\n",
    "            'payment_max_skill_multiplier': 190\n",
    "        }),\n",
    "        # (2) Taxing\n",
    "        ('DemocraticPeriodicBracketTax', {\n",
    "            'period':          2,\n",
    "            'bracket_spacing': 'us-federal',\n",
    "            'usd_scaling':     100,\n",
    "            'disable_taxes':   False\n",
    "        })\n",
    "    ],\n",
    "    \n",
    "    # ===== SCENARIO CLASS ARGUMENTS =====\n",
    "    # (optional) kwargs that are added by the Scenario class (i.e. not defined in BaseEnvironment)\n",
    "    \n",
    "    # ===== STANDARD ARGUMENTS ======\n",
    "    # kwargs that are used by every Scenario class (i.e. defined in BaseEnvironment)\n",
    "    'n_agents': 4,          # Number of non-planner agents (must be > 1)\n",
    "    'world_size': [1, 1], # [Height, Width] of the env world\n",
    "    'episode_length': 2, # Number of timesteps per episode\n",
    "    \n",
    "    # In multi-action-mode, the policy selects an action for each action subspace (defined in component code).\n",
    "    # Otherwise, the policy selects only 1 action.\n",
    "    'multi_action_mode_agents': True,\n",
    "    \n",
    "    # When flattening observations, concatenate scalar & vector observations before output.\n",
    "    # Otherwise, return observations with minimal processing.\n",
    "    'flatten_observations': False,\n",
    "    # When Flattening masks, concatenate each action subspace mask into a single array.\n",
    "    # Note: flatten_masks = True is required for masking action logits in the code below.\n",
    "    'flatten_masks': False,\n",
    "    \n",
    "    # How often to save the dense logs\n",
    "    'dense_log_frequency': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we have seen in earlier [tutorials](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb), using `env = foundation.make_env_instance(**env_config)` creates an environment instance `env` with the specified configuration.\n",
    "\n",
    "In order to use this environment with RLlib, we will also need to add the environment's `observation_space` and `action_space` attributes. Additionally, the environment itself must subclass the [`MultiAgentEnv`](https://github.com/ray-project/ray/blob/master/rllib/env/multi_agent_env.py) interface, which can return observations and rewards from multiple ready agents per step. To this end, we use an environment [wrapper](https://github.com/salesforce/ai-economist/blob/master/tutorials/rllib/env_wrapper.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EnvWrapper] Spaces\n",
      "[EnvWrapper] Obs (a)   \n",
      "DemocraticPeriodicBracketTax-curr_rates: (7,)\n",
      "DemocraticPeriodicBracketTax-is_first_day: (1,)\n",
      "DemocraticPeriodicBracketTax-is_tax_day: (1,)\n",
      "DemocraticPeriodicBracketTax-last_incomes: (4,)\n",
      "DemocraticPeriodicBracketTax-marginal_rate: (1,)\n",
      "DemocraticPeriodicBracketTax-tax_phase: (1,)\n",
      "SimpleLabor-skill: (1,)\n",
      "action_mask    : None\n",
      "time           : (1,)\n",
      "world-equality : (1,)\n",
      "world-normalized_per_capita_productivity: (1,)\n",
      "\n",
      "\n",
      "[EnvWrapper] Obs (p)   \n",
      "action_mask    : None\n",
      "\n",
      "\n",
      "[EnvWrapper] Action (a) MultiDiscrete([101  22  22  22  22  22  22  22])\n",
      "[EnvWrapper] Action (p) MultiDiscrete([1])\n"
     ]
    }
   ],
   "source": [
    "from rllib.env_wrapper import RLlibEnvWrapper\n",
    "env_obj = RLlibEnvWrapper({\"env_config_dict\": env_config_dict}, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon applying the wrapper to our environment, we have now defined observation and action spaces for the agents and the planner, indicated with `(a)` and `(p)` respectively. Also, (a useful tip) you can still access the environment instance and its attributes simply by using `env_obj.env`\n",
    "\n",
    "In summary, the observation spaces are represented as `Box` objects and the action spaces as `Discrete` objects (for more details on these types, see the OpenAI documentation [page](https://gym.openai.com/docs/#spaces)).\n",
    "\n",
    "Briefly looking at the shapes of the observation features (the numbers in parentheses), you will see that we have some one-dimensional features (e.g. `action-mask`, `flat`, `time`) as well as spatial features (e.g., `world-idx-map`, `world-map`)\n",
    "\n",
    "A couple of quick notes:\n",
    "- An `action_mask` is used to mask out the actions that are not allowed by the environment. For instance, a mobile agent cannot move beyond the boundary of the world. Hence, in position (0, 0), a mobile cannot move \"Left\" or \"Down\", and the corresponding actions in the mask would be nulled out. Now, the RL agent can still recommend to move \"Left\" or \"Down\", but the action isn't really taken.\n",
    "- The key `flat` arises since we set `flatten_observations': True`. Accordingly, the scalar and vector raw observations are all concatenated into this single key. If you're curious to see the entire set of raw observations, do set `flatten_observations': False` in the env_config, and re-run the above cell.\n",
    "\n",
    "Looking at the action spaces, the mobile agents can take 50 possible actions (including 1 NO-OP action or do nothing (always indexed 0), 44 trading-related actions, 4 move actions along the four directions and 1 build action)\n",
    "\n",
    "The planner sets the tax rates for 7 brackets, each from 0-100% in steps of 5%, so that's 21 values. Adding the NO-OP action brings the planner action space to `MultiDiscrete([22 22 22 22 22 22 22])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a *Trainer* Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our economic simulation environment with RLlib, you will need familiarity with one of the key classes: the [`Trainer`](https://docs.ray.io/en/master/rllib-training.html). The trainer object maintains the relationships that connect each agent in the environment to its corresponding trainable policy, and essentially helps in training, checkpointing policies and inferring actions. It helps to co-ordinate the workflow of collecting rollouts and optimizing the various policies via a reinforcement learning algorithm. Inherently, RLlib maintains a wide suite of [algorithms](https://docs.ray.io/en/master/rllib-algorithms.html) for multi-agent learning (which was another strong reason for us to consider using RLlib) - available options include SAC, PPO, PG, A2C, A3C, IMPALA, ES, DDPG, DQN, MARWIL, APEX, and APEX_DDPG. For the remainder of this tutorial, we will stick to using [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/) (PPO), an algorithm known to perform well generally.\n",
    "\n",
    "Every algorithm has a corresponding trainer object; in the context of PPO, we invoke the `PPOTrainer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPOTrainer can be instantiated with \n",
    "- `env`: an environment creator (i.e, RLlibEnvWrapper() in our case)\n",
    "- `config`: algorithm-specific configuration data for setting the various components of the RL training loop including the environment, rollout worker processes, training resources, degree of parallelism, framework used, and the policy exploration strategies.\n",
    "\n",
    "Note: There are several configuration settings related to policy architectures, rollout collection, minibatching, and other important hyperparameters, that need to be set carefully in order to train effectively. For the sake of the high-level exposition, we allow RLlib to use most of the the default settings. Check out the list of default [common configuration parameters](https://docs.ray.io/en/releases-0.8.4/rllib-training.html#common-parameters) and default [PPO-specific configuration parameters](https://docs.ray.io/en/releases-0.8.4/rllib-algorithms.html?highlight=PPO#proximal-policy-optimization-ppo). Custom environment configurations may be passed to environment creator via `config[\"env_config\"]`.\n",
    "\n",
    "RLlib also chooses default built-in [models](https://docs.ray.io/en/releases-0.8.4/rllib-models.html#built-in-models-and-preprocessors) for processing the observations. The models are picked based on a simple heuristic: a [vision](https://github.com/ray-project/ray/blob/master/rllib/models/tf/visionnet.py) network for observations that have shape of length larger than 2 (for example, (84 x 84 x 3)), and a [fully connected](https://github.com/ray-project/ray/blob/master/rllib/models/tf/fcnet.py) network for everything else. Custom models can be configured via the `config[\"policy\"][\"model\"]` key.\n",
    "\n",
    "In the context of multi-agent training, we will also need to set the multi-agent configuration:\n",
    "```python\n",
    "\"multiagent\": {\n",
    "        # Map of type MultiAgentPolicyConfigDict from policy ids to tuples\n",
    "        # of (policy_cls, obs_space, act_space, config). This defines the\n",
    "        # observation and action spaces of the policies and any extra config.\n",
    "        \"policies\": {},\n",
    "        # Function mapping agent ids to policy ids.\n",
    "        \"policy_mapping_fn\": None,\n",
    "        # Optional list of policies to train, or None for all policies.\n",
    "        \"policies_to_train\": None,\n",
    "    },\n",
    "```\n",
    "\n",
    "To this end, let's notate the agent policy id by `\"a\"` and the planner policy id by `\"p\"`. We can set `policies`, `policy_mapping_fun` and `policies_to_train` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = {\n",
    "    \"a\": (\n",
    "        None,  # uses default policy\n",
    "        env_obj.observation_space,\n",
    "        env_obj.action_space,\n",
    "        {}  # define a custom agent policy configuration.\n",
    "    ),\n",
    "    \"p\": (\n",
    "        None,  # uses default policy\n",
    "        env_obj.observation_space_pl,\n",
    "        env_obj.action_space_pl,\n",
    "        {}  # define a custom planner policy configuration.\n",
    "    )\n",
    "}\n",
    "\n",
    "# In foundation, all the agents have integer ids and the social planner has an id of \"p\"\n",
    "policy_mapping_fun = lambda i: \"a\" if str(i).isdigit() else \"p\"\n",
    "\n",
    "policies_to_train = [\"a\", \"p\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a multiagent trainer config holding the trainable policies and their mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policies_to_train\": policies_to_train,\n",
    "        \"policy_mapping_fn\": policy_mapping_fun,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With distributed RL, architectures typically comprise several **roll-out** and **trainer** workers operating in tandem\n",
    "![](assets/distributed_rl_architecture.png)\n",
    "\n",
    "The roll-out workers repeatedly step through the environment to generate and collect roll-outs in parallel, using the actions sampled from the policy models on the roll-out workers or provided by the trainer worker.\n",
    "Roll-out workers typically use CPU machines, and sometimes, GPU machines for richer environments.\n",
    "Trainer workers gather the roll-out data (asynchronously) from the roll-out workers and optimize policies on CPU or GPU machines.\n",
    "\n",
    "In this context, we can also add a `num_workers` configuration parameter to specify the number of rollout workers, i.e, those responsible for gathering rollouts. Note: setting `num_workers=0` will mean the rollouts will be collected by the trainer worker itself. Also, each worker can collect rollouts from multiple environments in parallel, which is specified in `num_envs_per_worker`; there will be a total of `num_workers` $\\times$ `num_envs_per_worker` environment replicas used to gather rollouts.\n",
    "Note: below, we also update some of the default trainer settings to keep the iteration time small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_config.update(\n",
    "    {\n",
    "        \"num_workers\": 2,\n",
    "        \"num_envs_per_worker\": 2,\n",
    "        # Other training parameters\n",
    "        \"train_batch_size\":  4000,\n",
    "        \"sgd_minibatch_size\": 2000,\n",
    "        \"num_sgd_iter\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to add the environment configuration to the trainer configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also add the \"num_envs_per_worker\" parameter for the env. wrapper to index the environments.\n",
    "env_config = {\n",
    "    \"env_config_dict\": env_config_dict,\n",
    "    \"num_envs_per_worker\": trainer_config.get('num_envs_per_worker'),   \n",
    "}\n",
    "\n",
    "trainer_config.update(\n",
    "    {\n",
    "        \"env_config\": env_config        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One the training configuration is set, we will need to initialize ray and create the PPOTrainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 23:07:53,084\tINFO resource_spec.py:212 -- Starting Ray with 3.08 GiB memory available for workers and up to 1.56 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2024-09-26 23:07:53,309\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.178.39',\n",
       " 'raylet_ip_address': '192.168.178.39',\n",
       " 'redis_address': '192.168.178.39:6379',\n",
       " 'object_store_address': 'tcp://127.0.0.1:53293',\n",
       " 'raylet_socket_name': 'tcp://127.0.0.1:57614',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': 'C:\\\\Users\\\\herrm\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2024-09-26_23-07-53_072021_18112'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import ray.dashboard\n",
    "\n",
    "# Initialize Ray\n",
    "ray.shutdown()\n",
    "ray.init(include_webui=True)\n",
    "\n",
    "# ray.start_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 22:59:59,963\tERROR syncer.py:46 -- Log sync requires rsync to be installed.\n",
      "2024-09-26 22:59:59,978\tINFO trainer.py:585 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2024-09-26 22:59:59,978\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\ray\\rllib\\models\\tf\\misc.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m Inside covid19_components.py: 0 GPUs are available.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m No GPUs found! Running the simulation on a CPU.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m Inside covid19_components.py: 0 GPUs are available.\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m No GPUs found! Running the simulation on a CPU.\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\ray\\rllib\\models\\tf\\misc.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "\u001b[2m\u001b[36m(pid=17812)\u001b[0m   out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m C:\\ProgramData\\anaconda3\\envs\\ai-economist\\lib\\site-packages\\ray\\rllib\\models\\tf\\misc.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "\u001b[2m\u001b[36m(pid=8764)\u001b[0m   out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 23:00:15,642\tINFO trainable.py:181 -- _setup took 15.664 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "# Create the PPO trainer.\n",
    "trainer = PPOTrainer(\n",
    "    env=RLlibEnvWrapper,\n",
    "    config=trainer_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We are now ready to perform training by invoking `trainer.train()`; we call it for just a few number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iter : 0 **********\n",
      "episode_reward_mean: -2520.2423514713105\n",
      "********** Iter : 1 **********\n",
      "episode_reward_mean: -2310.2691924659543\n",
      "********** Iter : 2 **********\n",
      "episode_reward_mean: -2208.7932692044333\n",
      "********** Iter : 3 **********\n",
      "episode_reward_mean: -1657.8760822063423\n",
      "********** Iter : 4 **********\n",
      "episode_reward_mean: -1052.0874144590116\n"
     ]
    }
   ],
   "source": [
    "NUM_ITERS = 5\n",
    "for iteration in range(NUM_ITERS):\n",
    "    print(f'********** Iter : {iteration} **********')\n",
    "    result = trainer.train()\n",
    "    print(f'''episode_reward_mean: {result.get('episode_reward_mean')}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the results will be logged to a subdirectory of `~/ray_results`. This subdirectory will contain a file `params.json` which contains the hyperparameters, a file `result.json` which contains a training summary for each episode and a TensorBoard file that can be used to visualize training process with TensorBoard by running|\n",
    "```shell\n",
    "tensorboard --logdir ~/ray_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate and Visualize the Environment's Dense Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any point during training, we would also want to inspect the environment's dense logs in order to deep-dive into the training results. Introduced in our [basic tutorial](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb#Visualize-using-dense-logging), dense logs are basically logs of each agent's states, actions and rewards at every point in time, along with a snapshot of the world state.\n",
    "\n",
    "There are two equivalent ways to fetch the environment's dense logs using the trainer object.\n",
    "\n",
    "a. Simply retrieve the dense log from the workers' environment objects\n",
    "\n",
    "b. Generate dense log(s) from the most recent trainer policy model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Simply retrieve the dense log from the workers' environment objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each rollout worker, it's straightforward to retrieve the dense logs using some of the function attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, we fetch the dense logs for each rollout worker and environment within\n",
    "\n",
    "dense_logs = {}\n",
    "# Note: worker 0 is reserved for the trainer actor\n",
    "for worker in range((trainer_config[\"num_workers\"] > 0), trainer_config[\"num_workers\"] + 1):\n",
    "    for env_id in range(trainer_config[\"num_envs_per_worker\"]):\n",
    "        dense_logs[\"worker={};env_id={}\".format(worker, env_id)] = \\\n",
    "        trainer.workers.foreach_worker(lambda w: w.async_env)[worker].envs[env_id].env.previous_episode_dense_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should have num_workers x num_envs_per_worker number of dense logs\n",
    "print(dense_logs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Generate a dense log from the most recent trainer policy model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also use the trainer object directly to play out an episode. The advantage of this approach is that we can re-sample the policy model any number of times and generate several rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rollout_from_current_trainer_policy(\n",
    "    trainer, \n",
    "    env_obj,\n",
    "    num_dense_logs=1\n",
    "):\n",
    "    dense_logs = {}\n",
    "    for idx in range(num_dense_logs):\n",
    "        # Set initial states\n",
    "        agent_states = {}\n",
    "        for agent_idx in range(env_obj.env.n_agents):\n",
    "            agent_states[str(agent_idx)] = trainer.get_policy(\"a\").get_initial_state()\n",
    "        planner_states = trainer.get_policy(\"p\").get_initial_state()   \n",
    "\n",
    "        # Play out the episode\n",
    "        obs = env_obj.reset(force_dense_logging=True)\n",
    "        for t in range(env_obj.env.episode_length):\n",
    "            actions = {}\n",
    "            for agent_idx in range(env_obj.env.n_agents):\n",
    "                # Use the trainer object directly to sample actions for each agent\n",
    "                \n",
    "                display(obs[str(agent_idx)])\n",
    "                \n",
    "                actions[str(agent_idx)] = trainer.compute_action(\n",
    "                    obs[str(agent_idx)], \n",
    "                    agent_states[str(agent_idx)], \n",
    "                    policy_id=\"a\",\n",
    "                    full_fetch=False\n",
    "                )\n",
    "\n",
    "            # Action sampling for the planner\n",
    "            actions[\"p\"] = trainer.compute_action(\n",
    "                obs['p'], \n",
    "                planner_states, \n",
    "                policy_id='p',\n",
    "                full_fetch=False\n",
    "            )\n",
    "\n",
    "            obs, rew, done, info = env_obj.step(actions)        \n",
    "            if done['__all__']:\n",
    "                break\n",
    "        dense_logs[idx] = env_obj.env.dense_log\n",
    "    return dense_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'world-normalized_per_capita_productivity': array([0.]),\n",
       " 'world-equality': array([1.]),\n",
       " 'time': array([0.]),\n",
       " 'SimpleLabor-skill': array([0.03737284]),\n",
       " 'DemocraticPeriodicBracketTax-is_tax_day': array([0.]),\n",
       " 'DemocraticPeriodicBracketTax-is_first_day': array([1.]),\n",
       " 'DemocraticPeriodicBracketTax-tax_phase': array([0.5]),\n",
       " 'DemocraticPeriodicBracketTax-last_incomes': array([0., 0., 0., 0.]),\n",
       " 'DemocraticPeriodicBracketTax-curr_rates': array([0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'DemocraticPeriodicBracketTax-marginal_rate': array([0.]),\n",
       " 'action_mask': {'SimpleLabor': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_000': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_097': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_394': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_842': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_1607': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_2041': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_5103': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'world-normalized_per_capita_productivity': array([0.]),\n",
       " 'world-equality': array([1.]),\n",
       " 'time': array([0.]),\n",
       " 'SimpleLabor-skill': array([0.08290421]),\n",
       " 'DemocraticPeriodicBracketTax-is_tax_day': array([0.]),\n",
       " 'DemocraticPeriodicBracketTax-is_first_day': array([1.]),\n",
       " 'DemocraticPeriodicBracketTax-tax_phase': array([0.5]),\n",
       " 'DemocraticPeriodicBracketTax-last_incomes': array([0., 0., 0., 0.]),\n",
       " 'DemocraticPeriodicBracketTax-curr_rates': array([0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'DemocraticPeriodicBracketTax-marginal_rate': array([0.]),\n",
       " 'action_mask': {'SimpleLabor': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_000': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_097': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_394': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_842': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_1607': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_2041': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_5103': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'world-normalized_per_capita_productivity': array([0.]),\n",
       " 'world-equality': array([1.]),\n",
       " 'time': array([0.]),\n",
       " 'SimpleLabor-skill': array([0.15151736]),\n",
       " 'DemocraticPeriodicBracketTax-is_tax_day': array([0.]),\n",
       " 'DemocraticPeriodicBracketTax-is_first_day': array([1.]),\n",
       " 'DemocraticPeriodicBracketTax-tax_phase': array([0.5]),\n",
       " 'DemocraticPeriodicBracketTax-last_incomes': array([0., 0., 0., 0.]),\n",
       " 'DemocraticPeriodicBracketTax-curr_rates': array([0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'DemocraticPeriodicBracketTax-marginal_rate': array([0.]),\n",
       " 'action_mask': {'SimpleLabor': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_000': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_097': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_394': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_842': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_1607': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_2041': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_5103': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'world-normalized_per_capita_productivity': array([0.]),\n",
       " 'world-equality': array([1.]),\n",
       " 'time': array([0.]),\n",
       " 'SimpleLabor-skill': array([0.31507185]),\n",
       " 'DemocraticPeriodicBracketTax-is_tax_day': array([0.]),\n",
       " 'DemocraticPeriodicBracketTax-is_first_day': array([1.]),\n",
       " 'DemocraticPeriodicBracketTax-tax_phase': array([0.5]),\n",
       " 'DemocraticPeriodicBracketTax-last_incomes': array([0., 0., 0., 0.]),\n",
       " 'DemocraticPeriodicBracketTax-curr_rates': array([0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'DemocraticPeriodicBracketTax-marginal_rate': array([0.]),\n",
       " 'action_mask': {'SimpleLabor': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_000': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_097': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_394': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_842': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_1607': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_2041': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'DemocraticPeriodicBracketTax.TaxIndexBracket_5103': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-dcc2d7abb3df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0menv_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mnum_dense_logs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32m<ipython-input-11-7fbe69546ee3>\u001b[0m in \u001b[0;36mgenerate_rollout_from_current_trainer_policy\u001b[1;34m(trainer, env_obj, num_dense_logs)\u001b[0m\n\u001b[0;32m     36\u001b[0m             )\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__all__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ai-economist\\tutorials\\rllib\\env_wrapper.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action_dict)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_agent_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"action_mask\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrecursive_list_to_np_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m F0926 19:36:19.664680 18912  8104 redis_client.cc:69]  Check failed: num_attempts < RayConfig::instance().redis_db_connect_retries() Expected 1 Redis shard addresses, found 2\r\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m *** Check failure stack trace: ***\r\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF7F35E608C  public: __cdecl google::LogMessage::~LogMessage(void) __ptr64\r\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF7F32FCFB4  public: virtual __cdecl google::NullStreamFatal::~NullStreamFatal(void) __ptr64\r\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF7F3555F8B  public: void __cdecl google::NullStreamFatal::`vbase destructor'(void) __ptr64\r\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF7F355524B  public: void __cdecl google::NullStreamFatal::`vbase destructor'(void) __ptr64\r\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF7F32DE419  public: class google::NullStream & __ptr64 __cdecl google::NullStream::stream(void) __ptr64\r\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF7F32D463B  public: class google::NullStream & __ptr64 __cdecl google::NullStream::stream(void) __ptr64\r\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF7F3293787  Ordinal0\r\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF7F39A43B5  bool __cdecl google::Demangle(char const * __ptr64,char * __ptr64,int)\r\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FFE63D17374  BaseThreadInitThunk\r\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FFE65C5CC91  RtlUserThreadStart\r\n"
     ]
    }
   ],
   "source": [
    "dense_logs = generate_rollout_from_current_trainer_policy(\n",
    "    trainer, \n",
    "    env_obj,\n",
    "    num_dense_logs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the episode dense logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we obtain the dense logs, we can use the plotting utilities we have created to examine the episode dense logs and visualize the the world state, agent-wise quantities, movement, and trading events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'social/productivity': 4904.4856900052055,\n",
       " 'social/equality': 0.7892415512525476,\n",
       " 'social_welfare/coin_eq_times_productivity': 967.7059735189074,\n",
       " 'social_welfare/inv_income_weighted_utility': -149.7279188662402,\n",
       " 'endow/avg_agent/Coin': 1226.1214225013014,\n",
       " 'endogenous/avg_agent/Labor': 35.0,\n",
       " 'util/avg_agent': -211.5532481824693,\n",
       " 'PeriodicTax/avg_bracket_rate/000': 0.2,\n",
       " 'PeriodicTax/bracket_occupancy/000': 0.0,\n",
       " 'PeriodicTax/avg_bracket_rate/097': 0.7000000000000001,\n",
       " 'PeriodicTax/bracket_occupancy/097': 0.25,\n",
       " 'PeriodicTax/avg_bracket_rate/394': 0.5,\n",
       " 'PeriodicTax/bracket_occupancy/394': 0.25,\n",
       " 'PeriodicTax/avg_bracket_rate/842': 0.6000000000000001,\n",
       " 'PeriodicTax/bracket_occupancy/842': 0.0,\n",
       " 'PeriodicTax/avg_bracket_rate/1607': 0.6000000000000001,\n",
       " 'PeriodicTax/bracket_occupancy/1607': 0.25,\n",
       " 'PeriodicTax/avg_bracket_rate/2041': 0.4,\n",
       " 'PeriodicTax/bracket_occupancy/2041': 0.25,\n",
       " 'PeriodicTax/avg_bracket_rate/5103': 0.0,\n",
       " 'PeriodicTax/bracket_occupancy/5103': 0.0,\n",
       " 'PeriodicTax/avg_effective_tax_rate': 0.5596333241780242,\n",
       " 'PeriodicTax/total_collected_taxes': 2746.293512984524,\n",
       " 'PeriodicTax/avg_tax_rate/poorest': 0.5600512837728583,\n",
       " 'PeriodicTax/avg_tax_rate/richest': 0.5545771660539709}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import plotting  # plotting utilities for visualizing env. state\n",
    "from ai_economist.foundation import utils\n",
    "\n",
    "dense_log_idx = 0\n",
    "env_obj.env.metrics\n",
    "#plotting.breakdown(utils.load_episode_log('rllib/phase1/dense_logs/logs_0000000002404800/env003.lz4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Ray after use\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it for now. See you in the [next](https://github.com/salesforce/ai-economist/blob/master/tutorials/two_level_curriculum_learning_with_rllib.md) tutorial :)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b139ae6237d52fdba9828784496281cf73a0aa758bee758129adc75b2e77b95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
